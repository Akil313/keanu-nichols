<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Kaggle: What’s Cooking | Keanu Nichols Blog</title>
<meta name="generator" content="Jekyll v3.7.3" />
<meta property="og:title" content="Kaggle: What’s Cooking" />
<meta name="author" content="Keanu Nichols" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Okay so welcome to our second blog entry on our solution to another Kaggle competition. This time we’re looking at a compeition called What’s Cooking, this one is pretty interesting well for me at least because it actually is in the realm of some NLP (Natural Language Processing). Well some of the aspects of it can be applied here, so some of the things that I came across while doing my stuff for GSoC (Google Summer of Code) is actually kind of helpful. So essentially our approach looked at using count vectorizer, tfidf (term frequency inverse document frequency) and looking t using different classification algorithms (best being Linear Support Vector Multiclassification)." />
<meta property="og:description" content="Okay so welcome to our second blog entry on our solution to another Kaggle competition. This time we’re looking at a compeition called What’s Cooking, this one is pretty interesting well for me at least because it actually is in the realm of some NLP (Natural Language Processing). Well some of the aspects of it can be applied here, so some of the things that I came across while doing my stuff for GSoC (Google Summer of Code) is actually kind of helpful. So essentially our approach looked at using count vectorizer, tfidf (term frequency inverse document frequency) and looking t using different classification algorithms (best being Linear Support Vector Multiclassification)." />
<link rel="canonical" href="http://localhost:4000/kaggle/2018/07/19/whats-cooking.html" />
<meta property="og:url" content="http://localhost:4000/kaggle/2018/07/19/whats-cooking.html" />
<meta property="og:site_name" content="Keanu Nichols Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-07-19T06:20:00-04:00" />
<script type="application/ld+json">
{"description":"Okay so welcome to our second blog entry on our solution to another Kaggle competition. This time we’re looking at a compeition called What’s Cooking, this one is pretty interesting well for me at least because it actually is in the realm of some NLP (Natural Language Processing). Well some of the aspects of it can be applied here, so some of the things that I came across while doing my stuff for GSoC (Google Summer of Code) is actually kind of helpful. So essentially our approach looked at using count vectorizer, tfidf (term frequency inverse document frequency) and looking t using different classification algorithms (best being Linear Support Vector Multiclassification).","author":{"@type":"Person","name":"Keanu Nichols"},"@type":"BlogPosting","url":"http://localhost:4000/kaggle/2018/07/19/whats-cooking.html","headline":"Kaggle: What’s Cooking","dateModified":"2018-07-19T06:20:00-04:00","datePublished":"2018-07-19T06:20:00-04:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/kaggle/2018/07/19/whats-cooking.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Keanu Nichols Blog" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Keanu Nichols Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/portfolio/">Porfolio</a><a class="page-link" href="/temp/">temp</a></div>
      </nav></div>
</header>
<!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Kaggle: What’s Cooking | Keanu Nichols Blog</title>
<meta name="generator" content="Jekyll v3.7.3" />
<meta property="og:title" content="Kaggle: What’s Cooking" />
<meta name="author" content="Keanu Nichols" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Okay so welcome to our second blog entry on our solution to another Kaggle competition. This time we’re looking at a compeition called What’s Cooking, this one is pretty interesting well for me at least because it actually is in the realm of some NLP (Natural Language Processing). Well some of the aspects of it can be applied here, so some of the things that I came across while doing my stuff for GSoC (Google Summer of Code) is actually kind of helpful. So essentially our approach looked at using count vectorizer, tfidf (term frequency inverse document frequency) and looking t using different classification algorithms (best being Linear Support Vector Multiclassification)." />
<meta property="og:description" content="Okay so welcome to our second blog entry on our solution to another Kaggle competition. This time we’re looking at a compeition called What’s Cooking, this one is pretty interesting well for me at least because it actually is in the realm of some NLP (Natural Language Processing). Well some of the aspects of it can be applied here, so some of the things that I came across while doing my stuff for GSoC (Google Summer of Code) is actually kind of helpful. So essentially our approach looked at using count vectorizer, tfidf (term frequency inverse document frequency) and looking t using different classification algorithms (best being Linear Support Vector Multiclassification)." />
<link rel="canonical" href="http://localhost:4000/kaggle/2018/07/19/whats-cooking.html" />
<meta property="og:url" content="http://localhost:4000/kaggle/2018/07/19/whats-cooking.html" />
<meta property="og:site_name" content="Keanu Nichols Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-07-19T06:20:00-04:00" />
<script type="application/ld+json">
{"description":"Okay so welcome to our second blog entry on our solution to another Kaggle competition. This time we’re looking at a compeition called What’s Cooking, this one is pretty interesting well for me at least because it actually is in the realm of some NLP (Natural Language Processing). Well some of the aspects of it can be applied here, so some of the things that I came across while doing my stuff for GSoC (Google Summer of Code) is actually kind of helpful. So essentially our approach looked at using count vectorizer, tfidf (term frequency inverse document frequency) and looking t using different classification algorithms (best being Linear Support Vector Multiclassification).","author":{"@type":"Person","name":"Keanu Nichols"},"@type":"BlogPosting","url":"http://localhost:4000/kaggle/2018/07/19/whats-cooking.html","headline":"Kaggle: What’s Cooking","dateModified":"2018-07-19T06:20:00-04:00","datePublished":"2018-07-19T06:20:00-04:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/kaggle/2018/07/19/whats-cooking.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Kaggle: What&#39;s Cooking</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-07-19T06:20:00-04:00" itemprop="datePublished">Jul 19, 2018
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><br />
Okay so welcome to our second blog entry on our solution to another <a href="https://www.kaggle.com/">Kaggle</a> competition. This time we’re looking at a compeition called <a href="https://www.kaggle.com/c/whats-cooking-kernels-only">What’s Cooking</a>, this one is pretty interesting well for me at least because it actually is in the realm of some NLP (Natural Language Processing). Well some of the aspects of it can be applied here, so some of the things that I came across while doing my stuff for GSoC (Google Summer of Code) is actually kind of helpful. So essentially our approach looked at using count vectorizer, tfidf (term frequency inverse document frequency) and looking t using different classification algorithms (best being Linear Support Vector Multiclassification).</p>

<h2 id="imports">Imports</h2>
<p>So first we go about importing our different modules, as usual we have our usual pandas and numpy. However our main focus will be on using sklearn (scikit-learn) and importing things such as CountVectorizer and LinearSVC.</p>

<figure class="highlight">
  <pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfTransformer</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">chi2</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span></pre></td></tr></tbody></table></code></pre>
</figure>

<h2 id="opening-files">Opening Files</h2>
<p>We then go about opening the “train.json” file and the “test.json” file. The data in these files are store as dictionaries.</p>

<figure class="highlight">
  <pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="code"><pre><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">"train.json"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">datastore</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
<span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">"test.json"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">test</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
<span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">datastore</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Number of recipes: "</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">datastore</span><span class="p">),</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">datastore</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s">'ingredients'</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">datastore</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">datastore</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">datastore</span><span class="p">)</span>
<span class="n">length1</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
<span class="nb">max</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">pos</span> <span class="o">=</span> <span class="mi">0</span></pre></td></tr></tbody></table></code></pre>
</figure>

<p>Resources:
<a href="https://github.com/arturomp/coursera-machine-learning-in-python/tree/master/mlclass-ex4-004/mlclass-ex4">Coursera to python</a></p>

<p><a href="https://www.coursera.org/learn/machine-learning">Andrew Ng Machine Learning</a></p>

<p>Files Used:<br />
Jupyter Notebook - <a href="https://github.com/kmn5409/Digit-Recognizer/blob/master/digit_recognizer_blog_code.ipynb">digit_recognizer_blog_code</a></p>


  </div><a class="u-url" href="/kaggle/2018/07/19/whats-cooking.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Keanu Nichols Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Keanu Nichols</li><li><a class="u-email" href="mailto:keanu.nichols@my.uwi.edu">keanu.nichols@my.uwi.edu</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/kmn5409"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">kmn5409</span></a></li><li><a href="https://www.linkedin.com/in/keanu-nichols"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">keanu-nichols</span></a></li><li><a href="/feed.xml"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#rss"></use></svg> <span>rss</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>A blog about Keanu Nichols for writing about his school/work experience and projects that he worked on.
</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
