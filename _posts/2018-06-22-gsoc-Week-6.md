---
layout: post
title: "GSoC Week 6: I need some space"
date: 2018-06-22T10:20:00Z
categories: GSoC
---
<br><br>
So it's my third post, maybe I'm actually going to keep this up, wouldn't that just be great, hopefully. I passed my first evaluation by the way! That made my day to be honest, especially because some people actually failed it due to issues with their mentor it seemed, so I'm very thankful that didn't happen to me. Now this blog post is about improving some stuff in both the PiperReader python file and the jupyter notebook. We're going to look at how to save some space, considering I didn't care about it before, but that could be an issue when you try to download for example 1GB worth of emails, yeah it's happened before. ALSO we will touch a little on some NLP (Natural Language Processing), sadly it's in the very early stages but I'm pretty excited for it, I hope it goes well.

So this week's task with my mentor [Sean Goggins](http://www.seangoggins.net/) is as follows:

# Goals for the week: 
1. Make table revisions to add columns for tracking message parts
2. Add a mailing list retrieval parameter to the augur.config.json file ... You can use the .git repository retrieval specification in examples to see how this could be formatted. You may need to ask derek or carter how to then have your program parse out mailing list retrieval
3. Include logic to keep track of the timestamp of the LAST message retrieved on each "load" .... add a "last_run" column to mailing_list_jobs, and then download only the mboxes not already included, and, for the oldest one, you will likely need to look for messages > the "last_run" column timestamp. All the UTC conversions need to occur before this comparison ...
4. Do a simple sentiment analysis on each message. Perhaps keep the "score" in an additional column in the table.
5. (Stretch Goal) : Experiment with ways of filtering out old message strings from mailing lists and identifying mailing list discussion threads (some lists have them automatically, some don't)


## Goals

Goal 1:<br>
Now what is this about? Well since in my last blog post we were looking at how to separate out really long posts and after talking with Sean he suggested that for people querying the SQL table having a counter as to how many lines the message is split up into and also what part of the message we're in. So I had to therefore add some columns to the table and you will see it below, the added columns are 'augurmsgID','message_part' and 'message_parts_tot'. 

{% highlight python linenos %}
df5.to_sql(name=db_name, con=db,if_exists='replace',index=False,
            dtype={'augurmsgID': s.types.Integer,
                'backend_name': s.types.VARCHAR(length=300),
                'project': s.types.VARCHAR(length=300),
                'mailing_list': s.types.VARCHAR(length=1000),
                'category': s.types.VARCHAR(length=300),
                'message_part': s.types.Integer,
                'message_parts_tot': s.types.Integer,
                'subject': s.types.VARCHAR(length=400),
                'date': s.types.DateTime(),
                'message_from': s.types.VARCHAR(length=500),
                'message_id': s.types.VARCHAR(length=500),
                'message_text': s.types.VARCHAR(length=12000)
            })
{% endhighlight %}

So 'message_parts_tot' is how many parts the message was split into as seen below (Line 14) is where I actually calculated it, the if statement is for when the 'mess_row_tot' went over
a multiple of 7000, since I was storing the message in dataframes of 7000 characters in length. For 'message_part' as seen below I used a variable called row_num (line 10) and set it to zero before the loop, and going through the iterations of the message I would increment it by 1 (line 18).

{% highlight python linenos %}
def add_row_mess(self,columns1,df,di,row,archives,augurmsgID):
    temp =  di['data']['body']['plain']
    words = ""
    k = 1
    val = False
    prev = 0
    length = len(temp)
    #print(length)
    mess_row_tot = 1
    row_num = 0
    if(length < 100):
        j = length
    else:
        mess_row_tot+= int(length/7000)
        if( (mess_row_tot*7000) > length ):
            mess_row_tot+=1
        for j in range(100,length,7000):
            row_num+=1
            k+=1
            date = self.convert_date(di['data']['Date'])
            li = [[augurmsgID,di['backend_name'],di['origin'],archives,
            di['category'], row_num, mess_row_tot, di['data']['Subject'],
            date, di['data']['From'],
            di['data']['Message-ID'],
            temp[prev:j] ]]
            df1 = pd.DataFrame(li,columns=columns1)
            df3 = df.append(df1)
            df = df3
            prev = j
            row+=1
            augurmsgID+=1
{% endhighlight %}

Also overall in the python file I tried to make it more readable, by putting everything in the class, I might have put the functions in a bit of a stranger way, I think when I go more into documentation I'll search up what is the conventional way.

Goal 2:<br>
I haven't started this as yet, I wanted to get more clarification about how to implement it, doesn't mean I was anywhere near to starting it though because I spent a bit of time on the next goal.

Goal 3:<br>
Now for this we're going to start with the jupyter notebook (PiperMail) since that's how it occurred to me and then I went to the python file after. So for this we want that if the user is first downloading the messages and uploads it to the SQL Database and created their table, they have in the table with only the mailing lists names called 'mailing_list_jobs' as seen below to store the most recent message they have. After say a day after they want to run the program again to download the most recent messages it only downloads and uploads the most recent message. Before I would keep downloading the messages, so we essentially want to save space because if they don't download messages for like a few months and decide to download them it could be quite a lot of messages to download.

So I first check to see if the SQL table 'mail_lists' which is where all the messages are downloaded. I think check if the table 'mailing_list_jobs' was created which is where I store the names of all the mailing lists and I take the names of the mailing lists; if it wasn't created then I create the table.

{% include PiperMail.md %}

The next part is where I'm going to download the mail archives. This is split into two parts, either the mailing list is not in the table in the SQL database so we have to download the archives and we use a dictionary 'mail_check' I made to say that it's 'new' (Lines 20 - 33). Or the mailing list is already in the database so we pull from the table 'mailing_list_jobs' and take the last_message_date column for that mailing list and check to see if there have been anymore recent messages, from [Perceval](https://github.com/chaoss/grimoirelab-perceval) there is a class called Pipermail that does this for us and if it has new messages we create the JSON file with the new messages and update that mailing list in the dictionary 'mail_check' to 'update' (Lines 34 - 54). Note that even if it has no recent messages at least the most recent mailing archive has to be downloaded because the class Pipermail has to check the messages in that archive.

{% include GSoC_Week6_Jupyter2.md %}

Below also is the outline of the 'mailing_list_jobs' table in the SQL database.

{%highlight python %}
df_mail_list.to_sql(name="mailing_list_jobs",con=db,if_exists='replace',index=False,
                    dtype={'backend_name': s.types.VARCHAR(length=300),
                            'mailing_list_url': s.types.VARCHAR(length=300),
                            'project': s.types.VARCHAR(length=300),
                            'last_message_date': s.types.DateTime()
                    })
{% endhighlight %}

We now go back to the PiperReader python file. So I'm mostly going to focus on these few lines of code because the rest of the program mainly stays the same. This essentially uses the 'mail_check' dictionary I created before and if it's key value is update then it reads the JSON file to pull the messages and upload it to the database. But if it's new then it takes the JSON file that was created with all the messages and uploads all of it. However it it's neither of these things and it's set to the default value I set as False, then it just goes onto the next mailing list. 

{% highlight python %}
for i in range(len(archives)):
    if(mail_check[archives[i]] == "update"):
        place = os.getcwd() + path + 'opendaylight-' + 'temp_' + archives[i]
    elif(mail_check[archives[i]] == 'new' ):
        place = os.getcwd() + path + 'opendaylight-' + archives[i]
        new = True
    else:
        print("Skipping")
        continue
{% endhighlight %}

I also made the function convert_date because I used it in the jupyter notebook and it just converts from a datetime object to a datetime64 object in UTC to be stored in pandas.

{% highlight python %}
def convert_date(self,di):
    split = di.split()
    sign = split[5][0]
    if sign == '-':
        sign = +1
    else:
        sign = -1
    hours = int(split[5][1:3]) * sign
    mins = int(split[5][3:6]) * sign
    s = " "
    date = parse(s.join(split[:5]))
    date = date + timedelta(hours = hours)
    date = date + timedelta(minutes = mins)
    return date
{% endhighlight %}

Now lets look at the Sentiment_Piper jupyter notebook, with this we start looking at the sentiment around the email. With this however you can see their are some problems with what NLTK (Natural Language Tool Kit) is looking at because for example in some of the emails there are error messages that it analyses which it doesn't need to. So just to talk about what I'm using essentially we use NLTK but we also use this tool called VADER which has already trained the NLP (Natural Language Processor) to score messages. So NLTK is breaking up the messages into smaller parts and analyzing it and giving you a score, one is the positive, negative, neutral and compound scores respectively. We are going to have to fine tune it a bit to ignore things such as errors for example and I'm going to have to look at deleting out some message threads.


{% include GSoC_Week_6_Sentiment_Reader_1.md %}





Resources:
https://programminghistorian.org/en/lessons/sentiment-analysis


Files Used:
Python File - [PiperRead 12](https://github.com/kmn5409/GSoC_CHAOSS/blob/master/Augur/Perceval/PiperReader%2012.py#L149)<br>
Jupyter Notebook - [PiperMail 4](https://github.com/kmn5409/GSoC_CHAOSS/blob/master/Augur/Perceval/PiperMail%204.ipynb)

Jupyter Notebook - [Sentiment_Piper 2](https://github.com/kmn5409/GSoC_CHAOSS/blob/master/Augur/Perceval/NLP/Sentiment_Piper%202.ipynb)


